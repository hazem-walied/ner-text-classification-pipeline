{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:10:04.465486Z","iopub.execute_input":"2025-04-18T12:10:04.465699Z","iopub.status.idle":"2025-04-18T12:10:06.555417Z","shell.execute_reply.started":"2025-04-18T12:10:04.465681Z","shell.execute_reply":"2025-04-18T12:10:06.554627Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport spacy\nfrom collections import Counter\nfrom tqdm.notebook import tqdm\n\n# Load spaCy for tokenization and lemmatization\nnlp = spacy.load(\"en_core_web_sm\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:10:28.948206Z","iopub.execute_input":"2025-04-18T12:10:28.948853Z","iopub.status.idle":"2025-04-18T12:10:38.753616Z","shell.execute_reply.started":"2025-04-18T12:10:28.948823Z","shell.execute_reply":"2025-04-18T12:10:38.752789Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class TextPreprocessor:\n    def __init__(self, remove_stopwords=True, lemmatize=True):\n        self.remove_stopwords = remove_stopwords\n        self.lemmatize = lemmatize\n    \n    def preprocess(self, text):\n        doc = nlp(text)\n        tokens = []\n        \n        for token in doc:\n            # Skip stopwords if configured\n            if self.remove_stopwords and token.is_stop:\n                continue\n            \n            # Skip punctuation\n            if token.is_punct:\n                continue\n                \n            # Lemmatize if configured, otherwise use the original token\n            processed_token = token.lemma_ if self.lemmatize else token.text\n            tokens.append(processed_token.lower())\n            \n        return tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:10:55.090884Z","iopub.execute_input":"2025-04-18T12:10:55.091589Z","iopub.status.idle":"2025-04-18T12:10:55.101241Z","shell.execute_reply.started":"2025-04-18T12:10:55.091560Z","shell.execute_reply":"2025-04-18T12:10:55.098874Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# NER Dataset\n","metadata":{}},{"cell_type":"code","source":"class NERDataset(Dataset):\n    def __init__(self, dataset_split, max_length=128):\n        self.dataset = dataset_split\n        self.max_length = max_length\n        \n        # Build vocabulary and tag dictionary\n        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n        self.tag2idx = {\"<PAD>\": 0}\n        self.idx2tag = {0: \"<PAD>\"}\n        \n        # Get tag names from dataset features\n        tag_names = dataset_split.features['ner_tags'].feature.names\n        for i, tag in enumerate(tag_names):\n            self.tag2idx[tag] = i + 1  # +1 because 0 is for PAD\n            self.idx2tag[i + 1] = tag\n        \n        # Build word vocabulary\n        word_counter = Counter()\n        for example in tqdm(dataset_split, desc=\"Building vocabulary\"):\n            for token in example['tokens']:\n                word_counter[token.lower()] += 1\n        \n        # Keep only words that appear at least 2 times\n        for word, count in word_counter.items():\n            if count >= 2:\n                self.word2idx[word] = len(self.word2idx)\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        tokens = example['tokens']\n        tags = example['ner_tags']\n        \n        # Convert tokens to indices\n        token_indices = []\n        for token in tokens[:self.max_length]:\n            token = token.lower()\n            if token in self.word2idx:\n                token_indices.append(self.word2idx[token])\n            else:\n                token_indices.append(self.word2idx[\"<UNK>\"])\n        \n        # Pad sequences\n        padding_length = self.max_length - len(token_indices)\n        if padding_length > 0:\n            token_indices = token_indices + [self.word2idx[\"<PAD>\"]] * padding_length\n            tags = tags[:self.max_length] + [0] * padding_length  # 0 is PAD tag\n        else:\n            token_indices = token_indices[:self.max_length]\n            tags = tags[:self.max_length]\n        \n        # Create attention mask (1 for real tokens, 0 for padding)\n        attention_mask = [1] * min(len(tokens), self.max_length) + [0] * padding_length\n        \n        return {\n            'input_ids': torch.tensor(token_indices, dtype=torch.long),\n            'tags': torch.tensor(tags, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:10:57.698167Z","iopub.execute_input":"2025-04-18T12:10:57.698724Z","iopub.status.idle":"2025-04-18T12:10:57.707740Z","shell.execute_reply.started":"2025-04-18T12:10:57.698703Z","shell.execute_reply":"2025-04-18T12:10:57.707093Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Text Classification Dataset\n","metadata":{}},{"cell_type":"code","source":"class TextClassificationDataset(Dataset):\n    def __init__(self, dataset_split, preprocessor=None, max_length=128):\n        self.dataset = dataset_split\n        self.preprocessor = preprocessor or TextPreprocessor()\n        self.max_length = max_length\n        \n        # Build vocabulary\n        self.word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n        \n        # Process all texts to build vocabulary\n        word_counter = Counter()\n        for example in tqdm(dataset_split, desc=\"Building vocabulary\"):\n            tokens = self.preprocessor.preprocess(example['text'])\n            for token in tokens:\n                word_counter[token] += 1\n        \n        # Keep only words that appear at least 5 times\n        for word, count in word_counter.items():\n            if count >= 5:\n                self.word2idx[word] = len(self.word2idx)\n        \n        # Get class names\n        self.num_classes = len(dataset_split.features['label'].names)\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, idx):\n        example = self.dataset[idx]\n        text = example['text']\n        label = example['label']\n        \n        # Preprocess text\n        tokens = self.preprocessor.preprocess(text)\n        \n        # Convert tokens to indices\n        token_indices = []\n        for token in tokens[:self.max_length]:\n            if token in self.word2idx:\n                token_indices.append(self.word2idx[token])\n            else:\n                token_indices.append(self.word2idx[\"<UNK>\"])\n        \n        # Pad sequences\n        padding_length = self.max_length - len(token_indices)\n        if padding_length > 0:\n            token_indices = token_indices + [self.word2idx[\"<PAD>\"]] * padding_length\n        else:\n            token_indices = token_indices[:self.max_length]\n        \n        # Create attention mask (1 for real tokens, 0 for padding)\n        attention_mask = [1] * min(len(tokens), self.max_length) + [0] * padding_length\n        \n        return {\n            'input_ids': torch.tensor(token_indices, dtype=torch.long),\n            'label': torch.tensor(label, dtype=torch.long),\n            'attention_mask': torch.tensor(attention_mask, dtype=torch.long)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:11:06.769758Z","iopub.execute_input":"2025-04-18T12:11:06.770265Z","iopub.status.idle":"2025-04-18T12:11:06.777877Z","shell.execute_reply.started":"2025-04-18T12:11:06.770244Z","shell.execute_reply":"2025-04-18T12:11:06.777186Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Create data loaders\n","metadata":{}},{"cell_type":"code","source":"def create_data_loaders(train_dataset, val_dataset, batch_size=32):\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2\n    )\n    \n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=2\n    )\n    \n    return train_loader, val_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:11:35.072375Z","iopub.execute_input":"2025-04-18T12:11:35.073236Z","iopub.status.idle":"2025-04-18T12:11:35.079252Z","shell.execute_reply.started":"2025-04-18T12:11:35.073199Z","shell.execute_reply":"2025-04-18T12:11:35.078154Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load NER dataset\nner_dataset = load_dataset(\"conll2003\")\n\n# Load text classification dataset\ntext_classification_dataset = load_dataset(\"ag_news\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:11:38.273634Z","iopub.execute_input":"2025-04-18T12:11:38.273936Z","iopub.status.idle":"2025-04-18T12:11:53.186485Z","shell.execute_reply.started":"2025-04-18T12:11:38.273913Z","shell.execute_reply":"2025-04-18T12:11:53.185653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/12.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dba35f0f8b2243d0a23c947125fe8abf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"conll2003.py:   0%|          | 0.00/9.57k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c516bfb5c3f8497ca5565d3334d9991b"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57374fdeebf54641975630db9afeccbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/14041 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26f86f405974d1c9f6e5338f209c858"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3250 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"445e517b306f4db18130216515a58409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3453 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e435f36e16e458c89f566cc9c71c18b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c30802180c6462c85f410abbe706bd5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b168765cfd54e7fb2ae59c76f84174c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18336ba450b0440192587398a53ca681"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a908477caa9a4151ad0bc2e2d701ed91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59363beac27449e5babfe2615def4f5a"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# Test the datasets\n","metadata":{}},{"cell_type":"code","source":"ner_train_dataset = NERDataset(ner_dataset['train'])\nner_val_dataset = NERDataset(ner_dataset['validation'])\n\ntext_preprocessor = TextPreprocessor(remove_stopwords=True, lemmatize=True)\ntext_train_dataset = TextClassificationDataset(text_classification_dataset['train'], preprocessor=text_preprocessor)\ntext_val_dataset = TextClassificationDataset(text_classification_dataset['test'], preprocessor=text_preprocessor)\n\nprint(f\"NER vocabulary size: {len(ner_train_dataset.word2idx)}\")\nprint(f\"NER tag set size: {len(ner_train_dataset.tag2idx)}\")\nprint(f\"Text classification vocabulary size: {len(text_train_dataset.word2idx)}\")\nprint(f\"Number of classes: {text_train_dataset.num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:11:53.187772Z","iopub.execute_input":"2025-04-18T12:11:53.188001Z","iopub.status.idle":"2025-04-18T12:33:34.286859Z","shell.execute_reply.started":"2025-04-18T12:11:53.187981Z","shell.execute_reply":"2025-04-18T12:33:34.286100Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/14041 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e949d0c033a480f891145a9c1937576"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/3250 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"362ce349c780434586decbe5c9f98529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/120000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"157e3c426f524caba5b26478198cb92d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Building vocabulary:   0%|          | 0/7600 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"278895e1d21c4b9bb2a960027724ffb2"}},"metadata":{}},{"name":"stdout","text":"NER vocabulary size: 10951\nNER tag set size: 10\nText classification vocabulary size: 24429\nNumber of classes: 4\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Sample a batch\n","metadata":{}},{"cell_type":"code","source":"ner_train_loader, ner_val_loader = create_data_loaders(ner_train_dataset, ner_val_dataset)\ntext_train_loader, text_val_loader = create_data_loaders(text_train_dataset, text_val_dataset)\n\nsample_ner_batch = next(iter(ner_train_loader))\nsample_text_batch = next(iter(text_train_loader))\n\nprint(\"NER batch shape:\")\nprint(f\"Input IDs: {sample_ner_batch['input_ids'].shape}\")\nprint(f\"Tags: {sample_ner_batch['tags'].shape}\")\nprint(f\"Attention mask: {sample_ner_batch['attention_mask'].shape}\")\n\nprint(\"\\nText classification batch shape:\")\nprint(f\"Input IDs: {sample_text_batch['input_ids'].shape}\")\nprint(f\"Labels: {sample_text_batch['label'].shape}\")\nprint(f\"Attention mask: {sample_text_batch['attention_mask'].shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-18T12:37:06.792467Z","iopub.execute_input":"2025-04-18T12:37:06.793090Z","iopub.status.idle":"2025-04-18T12:37:07.784617Z","shell.execute_reply.started":"2025-04-18T12:37:06.793062Z","shell.execute_reply":"2025-04-18T12:37:07.783781Z"}},"outputs":[{"name":"stdout","text":"NER batch shape:\nInput IDs: torch.Size([32, 128])\nTags: torch.Size([32, 128])\nAttention mask: torch.Size([32, 128])\n\nText classification batch shape:\nInput IDs: torch.Size([32, 128])\nLabels: torch.Size([32])\nAttention mask: torch.Size([32, 128])\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}